{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daea5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import helper\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa85b9d",
   "metadata": {},
   "source": [
    "First, load unique data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d53b1aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = helper.load_data_from_file(\"../data/graph_data_cleaned.json\") # dictionary of SMILES and values are graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1f9f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the unique atom types, orbitals and their respective binding energies\n",
    "\n",
    "atoms = []\n",
    "charges = []\n",
    "orbs = []\n",
    "energies = []\n",
    "\n",
    "\n",
    "for mol in train_data:\n",
    "    graph = train_data[mol]\n",
    "    \n",
    "    atom = list(nx.get_node_attributes(graph, \"atom_type\").values())\n",
    "    charge = list(nx.get_node_attributes(graph, \"formal_charge\").values())\n",
    "    orb = list(nx.get_node_attributes(graph, \"orbitals\").values())\n",
    "    energy = list(nx.get_node_attributes(graph, \"binding_energies\").values())\n",
    "    \n",
    "    for i in range(len(orb)):\n",
    "        for j in range(len(orb[i])):\n",
    "            if energy[i][j]!=-1:\n",
    "                atoms.append(atom[i])\n",
    "                charges.append(charge[i])\n",
    "                orbs.append(orb[i][j])\n",
    "                energies.append(energy[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3694ee",
   "metadata": {},
   "source": [
    "Encode these data points, both atomic number and quantum numbers for orbitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58ffa7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_dict = {\n",
    " 'Li': 3,\n",
    " 'B': 5,\n",
    " 'C': 6,\n",
    " 'N': 7,\n",
    " 'O': 8,\n",
    " 'F': 9,\n",
    " 'Ne': 10,\n",
    " 'Na': 11,\n",
    " 'Mg': 12,\n",
    " 'Al': 13,\n",
    " 'Si': 14,\n",
    " 'P': 15,\n",
    " 'S': 16,\n",
    " 'Cl': 17,\n",
    " 'Ar': 18,\n",
    " 'K': 19,\n",
    " 'Ca': 20,\n",
    " 'Ti': 22,\n",
    " 'V': 23,\n",
    " 'Cr': 24,\n",
    " 'Mn': 25,\n",
    " 'Fe': 26,\n",
    " 'Co': 27,\n",
    " 'Ni': 28,\n",
    " 'Cu': 29,\n",
    " 'Zn': 30,\n",
    " 'Ga': 31,\n",
    " 'Ge': 32,\n",
    " 'As': 33,\n",
    " 'Se': 34,\n",
    " 'Br': 35,\n",
    " 'Kr': 36,\n",
    " 'Rb': 37,\n",
    " 'Sr': 38,\n",
    " 'Mo': 42,\n",
    " 'Rh': 45,\n",
    " 'Ag': 47,\n",
    " 'Cd': 48,\n",
    " 'In': 49,\n",
    " 'Sn': 50,\n",
    " 'Sb': 51,\n",
    " 'Te': 52,\n",
    " 'I': 53,\n",
    " 'Xe': 54,\n",
    " 'Cs': 55,\n",
    " 'Ba': 56,\n",
    " 'W': 74,\n",
    " 'Re': 75,\n",
    " 'Hg': 80,\n",
    " 'Pb': 82,\n",
    " 'Bi': 83,\n",
    " 'U': 92\n",
    "}\n",
    "\n",
    "orb_dict = {\n",
    " '2p': [2, 1, 0],\n",
    " '2p3/2': [2, 1, 1.5],\n",
    " '4p3/2': [4, 1, 1.5],\n",
    " '3d5/2': [3, 2, 2.5],\n",
    " '3d': [3, 2, 0],\n",
    " '5p3/2': [5, 1, 1.5],\n",
    " '2s': [2, 0, 0],\n",
    " '1s': [1, 0, 0],\n",
    " '4s': [4, 0, 0],\n",
    " '3p': [3, 1, 0],\n",
    " '5d5/2': [5, 2, 2.5],\n",
    " '5s': [5, 0, 0],\n",
    " '3s': [3, 0, 0],\n",
    " '3p3/2': [3, 1, 1.5],\n",
    " '4d5/2': [4, 2, 2.5],\n",
    " '4d': [4, 2, 0],\n",
    " '4f7/2': [4, 3, 3.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74fba627",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for i in range(len(atoms)):\n",
    "    atom = atoms[i]\n",
    "    charge = charges[i]\n",
    "    orb = orbs[i]\n",
    "    energy = energies[i]\n",
    "    \n",
    "    atomic_number = atom_dict[atom]\n",
    "    q_numbers = orb_dict[orb]\n",
    "    \n",
    "    data.append([atomic_number]+[charge]+q_numbers+[energy])\n",
    "    \n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ac284",
   "metadata": {},
   "source": [
    "Let us split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9432cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = train_test_split(data, train_size=0.8)\n",
    "data_train = np.float_(data_train)\n",
    "data_test = np.float_(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc580ce",
   "metadata": {},
   "source": [
    "First, the training - let us split into input and output vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95d77bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data_train[:,:-1]\n",
    "y_train = data_train[:,-1]\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "# z-scale\n",
    "mu = np.mean(y_train)\n",
    "sig = np.std(y_train)\n",
    "\n",
    "y_train = (y_train-mu)/sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524228df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)        \n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c0b64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = 5\n",
    "hidden_size = 30\n",
    "output_size = 1\n",
    "batch_size = 8\n",
    "learning_rate = 1e-5\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d2a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fb3de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorDataset and DataLoader\n",
    "dataset = TensorDataset(torch.tensor(X_train), torch.tensor(y_train))\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68f2e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6261\n",
      "Epoch 26, Loss: 0.9726\n",
      "Epoch 51, Loss: 0.9503\n",
      "Epoch 76, Loss: 0.9338\n",
      "Epoch 101, Loss: 0.9180\n",
      "Epoch 126, Loss: 0.8967\n",
      "Epoch 151, Loss: 0.8771\n",
      "Epoch 176, Loss: 0.8595\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader, 0):\n",
    "        \n",
    "        inputs = inputs.float()  # now FloatTensor\n",
    "        labels = labels.float()\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    if epoch%25==0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbce33",
   "metadata": {},
   "source": [
    "Evaluate model predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa4a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = data_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ded8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal batch prediction approach\n",
    "# Kernel dies in the simplest approach\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a reasonable batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Create empty array for predictions\n",
    "num_samples = X_test.shape[0]\n",
    "y_pred_norm = np.zeros((num_samples, 1))\n",
    "\n",
    "# Process in batches\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    end_idx = min(i + batch_size, num_samples)\n",
    "    batch_X = X_test[i:end_idx]\n",
    "    \n",
    "    # Convert to tensor, ensure float type\n",
    "    batch_tensor = torch.tensor(batch_X, dtype=torch.float32)\n",
    "    \n",
    "    # Make prediction with no gradient tracking\n",
    "    with torch.no_grad():\n",
    "        y_pred_norm[i:end_idx] = model(batch_tensor).numpy()\n",
    "\n",
    "# Convert predictions back from log10 scale\n",
    "y_pred_test = sig*y_pred_norm + mu\n",
    "\n",
    "# Get actual test values and evaluate\n",
    "y_test = data_test[:, -1].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a85edcba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37639.05472983625"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y_test-y_pred_test)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62082d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
